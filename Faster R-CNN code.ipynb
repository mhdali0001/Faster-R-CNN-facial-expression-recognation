{"cells":[{"cell_type":"markdown","source":[" Detecton2 kütüphanesi ve gereken kütüphaneler indirmek kullanan bir code parçası"],"metadata":{"id":"CHKmAzXkiDOT"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"DTMl_TU-FNHV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705161732782,"user_tz":-180,"elapsed":22118,"user":{"displayName":"mhd ali","userId":"02133925031831066501"}},"outputId":"5d9ca5e3-e059-4453-a41a-e5d9bf7fb73d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyyaml==5.1\n","  Downloading PyYAML-5.1.tar.gz (274 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/274.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/274.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.2/274.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n","Cloning into 'detectron2'...\n","remote: Enumerating objects: 15303, done.\u001b[K\n","remote: Counting objects: 100% (28/28), done.\u001b[K\n","remote: Compressing objects: 100% (27/27), done.\u001b[K\n","remote: Total 15303 (delta 9), reused 11 (delta 1), pack-reused 15275\u001b[K\n","Receiving objects: 100% (15303/15303), 6.18 MiB | 10.93 MiB/s, done.\n","Resolving deltas: 100% (11124/11124), done.\n","Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (9.4.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (2.0.7)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (2.4.0)\n","Collecting yacs>=0.1.8\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (2.2.1)\n","Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.1)\n","Collecting fvcore<0.1.6,>=0.1.5\n","  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting iopath<0.1.10,>=0.1.7\n","  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n","Collecting omegaconf<2.4,>=2.1\n","  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting hydra-core>=1.1\n","  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting black\n","  Downloading black-23.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs>=0.1.8) (6.0.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.60.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.5.1)\n","Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.1)\n","Collecting portalocker (from iopath<0.1.10,>=0.1.7)\n","  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Collecting antlr4-python3-runtime==4.9.* (from omegaconf<2.4,>=2.1)\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.7)\n","Collecting mypy-extensions>=0.4.3 (from black)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Collecting pathspec>=0.9.0 (from black)\n","  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n","Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (4.1.0)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.0.1)\n","Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black) (4.5.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n","Building wheels for collected packages: fvcore, antlr4-python3-runtime\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=e40eaaf9c55069a2d9681c68def137566ada28dd21c42d7a5299c86a3c54db2b\n","  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=7d279f9efd87ad75d8c5710f614043e501a7e2bdd38ac3bcd04b73429eac9f5a\n","  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n","Successfully built fvcore antlr4-python3-runtime\n","Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore\n","Successfully installed antlr4-python3-runtime-4.9.3 black-23.12.1 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.12.1 portalocker-2.8.2 yacs-0.1.8\n"]}],"source":["!pip install pyyaml==5.1\n","\n","import sys, os, distutils.core\n","# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n","# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n","!git clone 'https://github.com/facebookresearch/detectron2'\n","dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n","!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n","sys.path.insert(0, os.path.abspath('./detectron2'))"]},{"cell_type":"markdown","source":[" gereken kütüphaneler tanımlamak kullanan bir code parçası"],"metadata":{"id":"H-A1BVSBiDJV"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"yzG67GXNFaz5","executionInfo":{"status":"ok","timestamp":1705161735377,"user_tz":-180,"elapsed":2597,"user":{"displayName":"mhd ali","userId":"02133925031831066501"}}},"outputs":[],"source":["# Some basic setup:\n","# Setup detectron2 logger\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","\n","# import some common libraries\n","import numpy as np\n","import os, json, cv2, random\n","from google.colab.patches import cv2_imshow\n","\n","# import some common detectron2 utilities\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog, DatasetCatalog"]},{"cell_type":"markdown","source":["#Tanılan görüntü hangi sınıfın olduğunu belirler"],"metadata":{"id":"f9b5neyYj8jt"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"PIbAM2pv-urF","executionInfo":{"status":"ok","timestamp":1705161755191,"user_tz":-180,"elapsed":1206,"user":{"displayName":"mhd ali","userId":"02133925031831066501"}}},"outputs":[],"source":["\n","from detectron2.structures import BoxMode\n","from PIL import Image\n","\n","def get_balloon_dicts(img_dir):\n","    json_file = os.path.join(img_dir, \"via_region_data.json\")\n","    with open(json_file) as f:\n","        imgs_anns = json.load(f)\n","\n","    dataset_dicts = []\n","    for idx, v in enumerate(imgs_anns.values()):\n","        record = {}\n","\n","        filename = os.path.join(img_dir, v[\"filename\"])\n","        if not os.path.exists(filename):\n","            print(f\"File not found: {filename}\")\n","            continue  # or handle the missing file in an appropriate way\n","\n","        height, width = cv2.imread(filename).shape[:2]\n","        record[\"file_name\"] = filename\n","        record[\"image_id\"] = idx\n","        record[\"height\"] = height\n","        record[\"width\"] = width\n","        annos = v[\"regions\"]\n","        objs = []\n","        for _, anno in annos.items():\n","            category = anno[\"region_attributes\"]\n","            shape = anno[\"shape_attributes\"]\n","            px = shape[\"x\"]\n","            py = shape[\"y\"]\n","            pwidth = shape[\"width\"]\n","            pheight = shape[\"height\"]\n","            pxn = [px, px+pwidth ]\n","            pyn = [py, py+pheight]\n","            if(\"Happy\" == category[\"Name\"]):\n","              categoryid = 0\n","            elif(\"Sad\" == category[\"Name\"]):\n","              categoryid = 1\n","            elif(\"Anger\" == category[\"Name\"]):\n","              categoryid = 2\n","            elif(\"Fear\" == category[\"Name\"]):\n","              categoryid = 3\n","            elif(\"Surprise\" == category[\"Name\"] or \"surprise\" == category[\"Name\"] ):\n","              categoryid = 4\n","            obj = {\n","                \"bbox\": [np.min(pxn), np.min(pyn), np.max(pxn), np.max(pyn)],\n","                \"bbox_mode\": BoxMode.XYXY_ABS,\n","                \"category_id\": categoryid,\n","            }\n","            objs.append(obj)\n","        record[\"annotations\"] = objs\n","        dataset_dicts.append(record)\n","    return dataset_dicts\n","\n","for d in [\"val\"]:\n","  DatasetCatalog.register(\"Face_\" +d, lambda d=d: get_balloon_dicts(\"/content/drive/MyDrive/data/\" + d))\n","MetadataCatalog.get(\"Face_val\").thing_classes = [\"Happy\",\"Sad\",\"Anger\",\"Fear\",\"Surprise\"]\n","MetadataCatalog.get(\"Face_train\").thing_classes = [\"Happy\",\"Sad\",\"Anger\",\"Fear\",\"Surprise\"]\n","balloon_metadata = MetadataCatalog.get(\"Face_val\")"]},{"cell_type":"markdown","metadata":{"id":"ObqdVJsysfdL"},"source":[" Modul tanımlamak için kullanan bir code parçası"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"W0QRoAFE8971","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705161780849,"user_tz":-180,"elapsed":21945,"user":{"displayName":"mhd ali","userId":"02133925031831066501"}},"outputId":"45585bec-7f32-4d4c-c311-42122335ad35"},"outputs":[{"output_type":"stream","name":"stdout","text":["[01/13 16:02:41 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from /content/drive/MyDrive/Faster RCNN/model_final.pth ...\n"]}],"source":["# Inference should use the config with parameters that are used in training\n","# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n","from detectron2.engine import DefaultTrainer\n","\n","cfg = get_cfg()\n","#cfg.MODEL.DEVICE = \"cpu\"\n","cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\")) #faster_rcnn_R_50_FPN_3x\n","cfg.DATASETS.TRAIN = (\"Face_train\",)\n","cfg.DATASETS.TEST = ()\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7\n","cfg.DATALOADER.NUM_WORKERS = 2\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\")  # faster_rcnn_R_50_FPN_3x\n","cfg.SOLVER.STEPS = ()  # Adjust these values as needed        # do not decay learning rate\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 5  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n","cfg.SOLVER.IMS_PER_BATCH = 8  # Number of images per batch\n","cfg.SOLVER.BASE_LR = 0.0001  # Learning rate\n","cfg.SOLVER.MAX_ITER = 14000  # Maximum number of training iterations\n","cfg.SOLVER.STEPS = (6000, 9000)   # Learning rate steps, adjust as needed\n","cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512  # RoI heads batch size per image\n","\n","# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n","\n","\n","cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"/content/drive/MyDrive/Faster RCNN/model_final.pth\")  # path to the model we just trained\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3   # set a custom testing threshold\n","\n","predictor = DefaultPredictor(cfg)"]},{"cell_type":"markdown","source":["Çok ifadeli yüz ifade tanıması yada Çok yüzlü ifade tanıması için kullanan code"],"metadata":{"id":"iNxZvfEdkAWQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"p5ct-sSSCvpK"},"outputs":[],"source":["import cv2\n","\n","\n","imo = cv2.imread(\"65.png\")\n","outputso = predictor(imo)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n","print(outputso)\n","vo = Visualizer(imo[:, :, ::-1],\n","              metadata=balloon_metadata,\n","              scale=2.5,\n","             # instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",")\n","\n","outo= vo.draw_instance_predictions(outputso[\"instances\"].to(\"cpu\"))\n","cv2_imshow(outo.get_image()[:, :, ::-1])"]},{"cell_type":"markdown","source":["Çok ölçekli yüz ifade tanıması için kullanan code"],"metadata":{"id":"ZQKtcHvMj4jL"}},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1705161735379,"user":{"displayName":"mhd ali","userId":"02133925031831066501"},"user_tz":-180},"id":"Da_ClKCkHjPu"},"outputs":[],"source":["import cv2\n","from detectron2.utils.visualizer import Visualizer, ColorMode\n","\n","# Assuming class_mapping is defined somewhere in your code\n","class_mapping = {\n","    0: \"Happy\",\n","    1: \"Sad\",\n","    2: \"Anger\",\n","    3: \"Fear\",\n","    4: \"Surprise\",\n","}\n","\n","imo = cv2.imread(\"re.png\")\n","outputso = predictor(imo)\n","print(outputso)\n","# Get the highest predicted class for the entire image\n","highest_confidence_index = outputso[\"instances\"].get(\"scores\").argmax().item()\n","\n","highest_confidence_class = class_mapping[outputso[\"instances\"].get(\"pred_classes\")[highest_confidence_index].item()]\n","\n","print(\"Highest Confidence Class for the Entire Image:\", highest_confidence_class)\n","\n","# Create a new Instances object for the highest predicted class\n","highest_class_instances = outputso[\"instances\"][highest_confidence_index : highest_confidence_index + 1].to(\"cpu\")\n","\n","# Visualize the result with only the highest confidence class\n","vo = Visualizer(imo[:, :, ::-1], metadata=balloon_metadata, scale=0.5)\n","outo = vo.draw_instance_predictions(highest_class_instances)\n","cv2_imshow(outo.get_image()[:, :, ::-1])\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1HyKzNgYN8aCIsCZaU7DZh7Qx0S7HzBPx","timestamp":1704037137720}],"mount_file_id":"1JhxNKHCTp68GWzjuiIw19T6eG4HW8Pxb","authorship_tag":"ABX9TyOK/M7SyP1IMgy1fmpu8m7c"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}